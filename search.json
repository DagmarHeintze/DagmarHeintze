[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dagmar Heintze",
    "section": "",
    "text": "I am a doctoral student studying International Relations.  \nMy research topics are Conflict, Human Rights and International Law.\n\n- More Information\n\nInterests:\n\n  * Prevention of torture and inhuman and degrading behavior or punishment \n  * International human rights litigation\n\n- Contact me\n\n[dagmar.heintze@utdallas.edu](mailto:dagmar.heintze@utdallas.edu)\n\n[Website](https://dagmarheintze.github.io)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "A comparison of Breiman (2001) ans Shmueli (2010)\nBreiman (2001) and Shmueli (2010) promote a more inclusive approach to conducting statistical modeling, moving from merely explanatory models to algorithmic/predictive modeling approaches.\n\nBreiman (2001) argues that statistical modeling up until the publication of his article has primarily focused on one method of statistical modeling of data to draw conclusions - the usage of data models. These models assume that data is derived from a stochastic model, and conclusions are drawn from the models based on this assumption. Breiman (2001) argues that the primary focus on stochastic modeling strategies can lead to inaccurate conclusions and theoretical assumptions that do not accurately describe the processes they seek to explain. Instead of focusing almost exclusively on stochastic models, Breiman (2001) promotes the usage of algorithmic models that were used more frequently in other fields outside of statistics but allow for processing large and complex data sets and more reliable conclusions when working with smaller data sets. When algorithmic models are used, the models can achieve better predictions and better explain underlying mechanisms. Breiman (2001) highlights that different approaches, such as random forests, can lead to a significantly reduced error rate compared to stochastic approaches, such as logistic regression analyses. Researchers can test their findings for accuracy when running data models and comparing the results with predictions. However, Breiman (2001) does not advocate against using data models per se but instead encourages the usage of appropriate models for the question at hand.\n\nShmueli (2010) takes up Breiman’s (2001) argument by arguing that, while explanatory modeling is commonly used, predictive modeling continues to be mostly ignored in scientific attempts to develop theory. Shmueli (2010) argues that predictive modeling approaches and predictive testing can fulfill several relevant scientific functions that cannot be easily fulfilled by explanatory modeling alone. Predictive modeling allows for the assessment of possible causal mechanisms in large data sets, the development of new measures, the uncovering of complex data patterns, an assessment of the gap between theory and practice, permits the assessment of competing theories and allows for an evaluation of the predictive power of relationships that can be statistically measured. Shmueli (2010) further expands Breiman’s (2001) argument by discussing not only the two different approaches of data vs. algorithmic modeling but also including a discussion of the data analysis process and taking four aspects (causation-association, theory-data, retrospective-prospective, and bias-variance) into account. In line with Breiman (2001), Shmueli (2010) does not discount the value of explanatory modeling per se. While Breiman (2001) argues that statistical modeling approaches should be selected according to the problem at hand, Shmueli (2010) argues that a model’s explanatory and predictive power are two distinct qualities of a well-rounded model and, therefore, predictive modeling should be included and tested together with explanatory modeling approaches. Without predictive modeling, existing theories cannot be tested regarding their relevance, and new causal mechanisms are not identified. A model’s explanatory power is not equal to its predictive power, and without a separate assessment and comparison, researchers may draw inaccurate conclusions. Furthermore, Shmueli (2010) argues that by excluding predictive modeling approaches, a gap between theory and practice continues to exist, and scientific research advances are stumped, which could both be addressed by combining both modeling approaches. Furthermore, while a model may not be bad if it does not have predictive or explanatory power but rather emphasizes the alternative quality, both explanatory and predictive modeling results should be presented in the research to allow for a well-rounded assessment of the results.\n\nIn summary, both Breiman (2001) and Shmueli (2010) argue that explanatory modeling is not the only way to approach statistical modeling, but we should also consider predictive modeling. Both authors further argue that, as a scientific community, we should know which models serve what scientific purpose and be able to differentiate them and apply them accordingly. Only then can we minimize error and reliably test our theories."
  }
]